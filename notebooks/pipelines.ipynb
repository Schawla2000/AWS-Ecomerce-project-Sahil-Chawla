{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing tranformer notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9aab946-756f-46a2-8ed2-085cae6eb53b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bronze to Silver dataset tranformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "083ee6e4-a37a-4e86-8ef7-77851da1643c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class BronzeToSilverETL:\n",
    "    def __init__(self, bucket):\n",
    "        self.bucket=bucket\n",
    "\n",
    "    def run(self):\n",
    "        raw_path=\"s3://e-commerce-etl-yj\"\n",
    "        logger.info(\"Starting Bronze → Silver ETL\")\n",
    "\n",
    "        #customer\n",
    "        df_c=spark.read.csv(f\"{raw_path}/raw_data/customers.csv\",inferSchema=True,header=True)\n",
    "        df_c.show()\n",
    "        df_c_clean=CustomerTransformer().transform(df_c)\n",
    "        df_c_clean.write.mode(\"overwrite\").parquet(f\"{raw_path}/silver/customers_clean\")\n",
    "\n",
    "        #orders\n",
    "        df_o=spark.read.csv(f\"{raw_path}/raw_data/orders.csv\",inferSchema=True,header=True)\n",
    "        df_o.show()\n",
    "        df_o_clean=OrderTransformer().transform(df_o)\n",
    "        df_o_clean.write.mode(\"overwrite\").parquet(f\"{raw_path}/silver/orders_clean\")\n",
    "        \n",
    "\n",
    "        #products\n",
    "        df_p=spark.read.csv(f\"{raw_path}/raw_data/products.csv\",inferSchema=True,header=True)\n",
    "        df_p_clean=ProductTransformer().transform(df_p)\n",
    "        df_p_clean.write.mode(\"overwrite\").parquet(f\"{raw_path}/silver/products_clean\")\n",
    "\n",
    "        #payments\n",
    "        df_pay=spark.read.json(f\"{raw_path}/raw_data/payments.json\",multiLine=True)\n",
    "        df_pay_clean=PaymentTransformer().transform(df_pay)\n",
    "        df_pay_clean.write.mode(\"overwrite\").parquet(f\"{raw_path}/silver/payments_clean\")\n",
    "\n",
    "        logger.info(\"Bronze → Silver ETL Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silver to Gold dataset tranformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a39a563-ea34-49d0-a19a-6af0ec01e0f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, col, avg,min, countDistinct, max, desc, count, when\n",
    "\n",
    "class SilverToGoldETL:\n",
    "    def __init__(self,bucket):\n",
    "        self.bucket=bucket\n",
    "\n",
    "    def run(self):\n",
    "        raw_path=\"s3://e-commerce-etl-yj\"\n",
    "        logger.info(\"Starting Silver → Gold ETL\")\n",
    "\n",
    "        orders=spark.read.parquet(f\"{raw_path}/silver/orders_clean/\")\n",
    "        payments=spark.read.parquet(f\"{raw_path}/silver/payments_clean/\")\n",
    "        customers=spark.read.parquet(f\"{raw_path}/silver/customers_clean/\")\n",
    "        products=spark.read.parquet(f\"{raw_path}/silver/products_clean/\")\n",
    "        orders.show()\n",
    "\n",
    "        # 1 Daily Sales\n",
    "        daily_sales_df=orders.join(payments,\"order_id\",\"inner\").groupBy(\"order_date\").agg(sum(col(\"amount\")).alias(\"daily_sales\"),avg(\"amount\").alias(\"avg_sales\")).orderBy(\"order_date\")\n",
    "        \n",
    "        daily_sales_df.write.mode(\"overwrite\").parquet(f\"{raw_path}/gold/daily_sales/\")\n",
    "        #daily_sales_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"`e-commerce-project-data`.salesReport\")\n",
    "        \n",
    "        logger.info(\"Daily sales report table created\")\n",
    "\n",
    "        # 2 Customer Lifetime Value (CLV)\n",
    "        customer_clv_df=customers.join(orders,\"customer_id\",\"left\").join(payments,\"order_id\",\"left\").groupBy(\"customer_id\").agg(sum(\"amount\").alias(\"lifetime_value\"),countDistinct(\"order_id\").alias(\"orders_count\"),min(\"order_date\").alias(\"first_purchase\"),max(\"order_date\").alias(\"last_purchase\"))\n",
    "        customer_clv_df.write.mode(\"overwrite\").parquet(f\"{raw_path}/gold/customerCLVReport/\")\n",
    "        #customer_clv_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"`e-commerce-project-data`.customerCLVReport\")\n",
    "        logger.info(\"Customer Lifetime Value table created\")\n",
    "\n",
    "        # 3 Top Products\n",
    "        top_products_df=orders.join(payments,\"order_id\",\"inner\").groupBy(\"product_id\").agg(sum(\"amount\").alias(\"total_sales\"),count(\"*\").alias(\"units_sold\")).join(products.select(\"product_id\", \"name\", \"category\"), \"product_id\").orderBy(desc(\"total_sales\"))\n",
    "        top_products_df.write.mode(\"overwrite\").parquet(f\"{raw_path}/gold/topProducts/\")\n",
    "        #top_products_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"`e-commerce-project-data`.topProducts\")\n",
    "        logger.info(\"Top Products table created\")\n",
    "\n",
    "        # 4 Category Sales\n",
    "        category_sales = (\n",
    "            orders.join(payments, \"order_id\")\n",
    "                  .join(products, \"product_id\")\n",
    "                  .groupBy(\"category\")\n",
    "                  .agg(\n",
    "                      sum(\"amount\").alias(\"total_sales\"),\n",
    "                      countDistinct(\"order_id\").alias(\"total_orders\")\n",
    "                  )\n",
    "                  .orderBy(desc(\"total_sales\"))\n",
    "        )\n",
    "\n",
    "        category_sales.write.mode(\"overwrite\").parquet(f\"{raw_path}/gold/categorySales/\")\n",
    "        #category_sales.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"`e-commerce-project-data`.categorySales\")\n",
    "        logger.info(\"Category wise sales table created\")\n",
    "\n",
    "        # 5 Repeat Customer Rate\n",
    "        repeat_customers =orders.groupBy(\"customer_id\").agg(count(\"order_id\").alias(\"orders_count\"))\n",
    "        repeat_rate = repeat_customers.withColumn(\"is_repeat\", when(col(\"orders_count\") > 1, 1).otherwise(0)).agg((sum(\"is_repeat\")/count(\"customer_id\")*100).alias(\"repeat_customer_rate\"))\n",
    "        \n",
    "        repeat_rate.write.mode(\"overwrite\").parquet(f\"{raw_path}/gold/repeatRate/\")\n",
    "        #repeat_rate.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"`e-commerce-project-data`.repeatRate\")\n",
    "        logger.info(\"Repeat Customer Rate table created\")\n",
    "\n",
    "        logger.info(\"Silver → Gold ETL Completed\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pipelines",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
